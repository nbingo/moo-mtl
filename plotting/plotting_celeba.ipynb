{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "586e272b-3b26-4bb1-994d-284595e876f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "from pymoo.factory import get_performance_indicator\n",
    "\n",
    "#\n",
    "# Helper functions\n",
    "#\n",
    "\n",
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower() \n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(l, key = alphanum_key)\n",
    "\n",
    "\n",
    "def get_early_stop(epoch_data, key='hv'):\n",
    "    assert key in ['hv', 'score', 'last']\n",
    "    if key == 'hv':\n",
    "        best_hv = 0\n",
    "        best_idx = None\n",
    "        for e in natural_sort(epoch_data.keys()):\n",
    "            if 'epoch' in e and 'loss' in epoch_data[e]:\n",
    "                hv = epoch_data[e]['loss']['hv']\n",
    "                if hv > best_hv:\n",
    "                    best_hv = hv\n",
    "                    best_idx = e\n",
    "        return best_idx\n",
    "    elif key == 'score':\n",
    "        min_score = 1e15\n",
    "        min_epoch = -1\n",
    "        for e in natural_sort(epoch_data.keys()):\n",
    "            if 'scores' in epoch_data[e]:\n",
    "                s = epoch_data[e]['scores'][0]\n",
    "                s = s[epoch_data[e]['task']]\n",
    "\n",
    "                if s < min_score:\n",
    "                    min_score = s\n",
    "                    min_epoch = e\n",
    "\n",
    "        return int(min_epoch.replace('epoch_', ''))\n",
    "    elif key == 'last':\n",
    "        last_epoch = natural_sort(epoch_data.keys())[-1]\n",
    "        return int(last_epoch.replace('epoch_', ''))\n",
    "\n",
    "\n",
    "def load_files(paths):\n",
    "    contents = []\n",
    "    for p in paths:\n",
    "        with p.open(mode='r') as json_file:\n",
    "            contents.append(json.load(json_file))\n",
    "    return contents\n",
    "\n",
    "\n",
    "def mean_and_std(values):\n",
    "    return (\n",
    "        np.array(values).mean(axis=0).tolist(),\n",
    "        np.array(values).std(axis=0).tolist()\n",
    "    )\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return zip(a, a)\n",
    "\n",
    "\n",
    "def get_hv(sol):\n",
    "    hv = get_performance_indicator(\"hv\", ref_point=np.array([1, 1]))\n",
    "    return hv.do(np.array(sol))\n",
    "\n",
    "\n",
    "def adjust_lightness(color, amount=0.5):\n",
    "    import matplotlib.colors as mc\n",
    "    import colorsys\n",
    "    try:\n",
    "        c = mc.cnames[color]\n",
    "    except:\n",
    "        c = color\n",
    "    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n",
    "    return colorsys.hls_to_rgb(c[0], max(0, min(1, amount * c[1])), c[2])\n",
    "\n",
    "\n",
    "#\n",
    "# Plotting params\n",
    "#\n",
    "font_size = 12\n",
    "figsize=(14, 3.5)\n",
    "\n",
    "plt.rcParams.update({'font.size': font_size})\n",
    "plt.tight_layout()\n",
    "\n",
    "markers = {\n",
    "    'mgda': 'x', \n",
    "    'uniform': '^',\n",
    "    # 'phn': '.', \n",
    "    'cosmos': 'd', \n",
    "    # 'phn_orig': '.', \n",
    "    'cosmos_orig': 'd', \n",
    "    'pmtl': '*',\n",
    "    'linear_scalarization': '.',\n",
    "    'calibrated_linear_scalarization': '.',\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'single_task': '#1f77b4', \n",
    "    # 'mgda': '#ff7f0e', \n",
    "    # 'phn_orig': '#800000',\n",
    "    # 'phn': '#2ca02c',\n",
    "    'cosmos_orig': '#f58231',\n",
    "    'linear_scalarization': '#800000',\n",
    "    'calibrated_linear_scalarization': '#2ca02c',\n",
    "    'cosmos': '#d62728',\n",
    "    # 'pmtl': '#9467bd', \n",
    "    # 'uniform': '#f032e6', \n",
    "}\n",
    "\n",
    "titles = {\n",
    "    'multi_mnist': \"Multi-MNIST\", \n",
    "    'multi_fashion': 'Multi-Fashion',\n",
    "    'multi_fashion_mnist': 'Multi-Fashion+MNIST',\n",
    "    'celeba': 'Celeba',\n",
    "}\n",
    "\n",
    "ax_lables_loss = {\n",
    "    'multi_mnist': ('Cross-Entropy Loss Task TL', 'Cross-Entropy Loss Task BR'), \n",
    "    'multi_fashion': ('Cross-Entropy Loss Task TL', 'Cross-Entropy Loss Task BR'), \n",
    "    'multi_fashion_mnist': ('Cross-Entropy Loss Task TL', 'Cross-Entropy Loss Task BR'), \n",
    "    'celeba': ('Binary Cross-Entroy Loss Task 16', 'Binary Cross-Entroy Loss Task 22')\n",
    "}\n",
    "\n",
    "ax_lables_mcr = {\n",
    "    'multi_mnist': ('Misclassification Rate Task TL', 'Misclassification Rate Task BR'), \n",
    "    'multi_fashion': ('Misclassification Rate Task TL', 'Misclassification Rate Task BR'), \n",
    "    'multi_fashion_mnist': ('Misclassification Rate Task TL', 'Misclassification Rate Task BR'), \n",
    "    'celeba': ('Misclassification Rate Task 16', 'Misclassification Rate Task 22')\n",
    "}\n",
    "\n",
    "method_names = {\n",
    "    'single_task': 'Single Task', \n",
    "    'phn': 'PHN*',\n",
    "    'phn_orig': 'PHN',\n",
    "    'cosmos_orig': 'COSMOS',\n",
    "    'cosmos': 'COSMOS*',\n",
    "    'pmtl': 'PMTL', \n",
    "    'uniform': 'Uniform',\n",
    "    'mgda': 'MGDA',\n",
    "    'linear_scalarization': 'Linear Scalarization',\n",
    "    'calibrated_linear_scalarization': 'Calibrated Linear Scalarization'\n",
    "}\n",
    "\n",
    "limits_loss = {\n",
    "    # dataset: [left, right, bottom, top]\n",
    "    'multi_mnist': [.4, .4], \n",
    "    'multi_fashion': [.55, .55], \n",
    "    'multi_fashion_mnist': [.55, .55],\n",
    "    # 'celeba': [.25, .25]\n",
    "    'celeba': [.7, .7]\n",
    "}\n",
    "\n",
    "limits_mcr = {\n",
    "    # dataset: [left, bottom]\n",
    "    'multi_mnist': [.12, .12], \n",
    "    'multi_fashion': [.20, .20,], \n",
    "    'multi_fashion_mnist': [.3, .3],\n",
    "    # 'celeba': [.05, .05]\n",
    "    'celeba': [.6,.3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62cb6456-fd95-48ea-a504-a20e5bbb9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load the data\n",
    "#\n",
    "\n",
    "def load_data(\n",
    "    dirname='results', \n",
    "    datasets=['celeba'],\n",
    "    methods= ['cosmos_orig', 'single_task', 'linear_scalarization', 'calibrated_linear_scalarization'],\n",
    "    custom_metric=False,\n",
    "    ):\n",
    "\n",
    "    metric = 'metrics' if custom_metric else 'loss'\n",
    "\n",
    "    p = Path(dirname)\n",
    "    results = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        results[dataset] = {}\n",
    "        for method in methods:\n",
    "            val_file = list(sorted(p.glob(f'**/{method}/{dataset}/run_uncorr*/val*.json')))\n",
    "            test_file = list(sorted(p.glob(f'**/{method}/{dataset}/run_uncorr*/test*.json')))\n",
    "\n",
    "            if len(val_file) == 0:\n",
    "                continue\n",
    "            assert len(val_file) == len(test_file)\n",
    "\n",
    "            data_val = load_files(val_file)\n",
    "            data_test = load_files(test_file)\n",
    "\n",
    "            test_scores = []\n",
    "            test_hv = []\n",
    "            training_time = []\n",
    "\n",
    "            if method == 'single_task':\n",
    "                for (val_run_1, val_run_2), (test_run_1, test_run_2) in zip(pairwise(data_val), pairwise(data_test)):\n",
    "                    e1 = get_early_stop(val_run_1)\n",
    "                    e2 = get_early_stop(val_run_2)\n",
    "                    r1 = test_run_1[e1]\n",
    "                    r2 = test_run_2[e2]\n",
    "\n",
    "                    sol = [r1[metric]['center_ray'][0], r2[metric]['center_ray'][1]]\n",
    "                    test_scores.append(sol)\n",
    "                    test_hv.append(get_hv(sol))\n",
    "                    training_time.append(r1['training_time_so_far'] + r2['training_time_so_far'])\n",
    "\n",
    "                    test_run = test_run_1\n",
    "            else:\n",
    "                for val_run, test_run in zip(data_val, data_test):\n",
    "                    e = get_early_stop(val_run)\n",
    "                    r = test_run[e]\n",
    "                    test_scores.append(r[metric]['pareto_front'] if 'pareto_front' in r[metric] else r[metric]['center_ray'])\n",
    "                    test_hv.append(r[metric]['hv'])\n",
    "                    training_time.append(r['training_time_so_far'])\n",
    "\n",
    "            results[dataset][method] = {\n",
    "                'test_scores': mean_and_std(test_scores),\n",
    "                'test_hv': mean_and_std(test_hv),\n",
    "                'train_time': mean_and_std(training_time),\n",
    "                'num_parameters': test_run['num_parameters'],\n",
    "            }\n",
    "        print(f'loaded data for {dataset}')\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea151732-ee3a-4162-9682-7f131ea7ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_row(results, \n",
    "        datasets=['celeba'],\n",
    "        methods=['single_task', 'cosmos_orig', 'linear_scalarization', 'calibrated_linear_scalarization'],\n",
    "        prefix=''):\n",
    "    assert len(datasets) <= 3\n",
    "    fig, axes = plt.subplots(1, len(datasets), figsize=(4.5*len(datasets), 3.5))\n",
    "    for j, dataset in enumerate(datasets):\n",
    "        if dataset not in results:\n",
    "            continue\n",
    "        ax = axes[j] if isinstance(axes, list) else axes\n",
    "        lower_limit = None\n",
    "        for method in methods:\n",
    "            if method not in results[dataset]:\n",
    "                continue\n",
    "            r = results[dataset][method]\n",
    "            # we take the mean only\n",
    "            s = np.array(r['test_scores'][0])\n",
    "            if s.ndim == 1:\n",
    "                s = np.expand_dims(s, 0)\n",
    "            if method == 'single_task':\n",
    "                s = np.squeeze(s)\n",
    "                ax.axvline(x=s[0], color=colors[method], linestyle='-.')\n",
    "                ax.axhline(y=s[1], color=colors[method], linestyle='-.', label=\"{}\".format(method_names[method]))\n",
    "                lower_limit = s\n",
    "            else:\n",
    "                ax.plot(\n",
    "                    s[:, 0], \n",
    "                    s[:, 1], \n",
    "                    color=colors[method],\n",
    "                    marker=markers[method],\n",
    "                    linestyle='--' if method in ['phn', 'phn_orig', 'cosmos', 'cosmos_orig'] else ' ',\n",
    "                    label=\"{}\".format(method_names[method])\n",
    "                )\n",
    "        lim = limits_loss[dataset] if 'mcr' not in prefix else limits_mcr[dataset]\n",
    "        margin = .05 if 'mcr' not in prefix else 0.005\n",
    "        ax.set_xlim(left=lower_limit[0] - margin, right=lim[0])\n",
    "        ax.set_ylim(bottom=lower_limit[1] - margin, top=lim[1])\n",
    "        ax.set_title(titles[dataset])\n",
    "        ax_lables = ax_lables_loss[dataset] if 'mcr' not in prefix else ax_lables_mcr[dataset]\n",
    "        ax.set_xlabel(ax_lables[0])\n",
    "        if j==0:\n",
    "            ax.set_ylabel(ax_lables[1])\n",
    "\n",
    "        if j==0:\n",
    "            ax.legend(loc='upper right', bbox_to_anchor=(1, -0.23), fancybox=True)\n",
    "    plt.subplots_adjust(wspace=.25)\n",
    "    fig.savefig(prefix + '.pdf', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print('success. See', prefix + '.pdf')\n",
    "\n",
    "\n",
    "#\n",
    "# generating the tables\n",
    "#\n",
    "\n",
    "def generate_table(\n",
    "        results, \n",
    "        datasets=['multi_mnist', 'multi_fashion', 'multi_fashion_mnist'], \n",
    "        methods=['single_task', 'uniform', 'mgda', 'pmtl', 'phn', 'phn_orig', 'cosmos', 'cosmos_orig', ],\n",
    "        name='test'\n",
    "    ):\n",
    "    text = f\"\"\"\"\"\"\n",
    "    for method in methods:\n",
    "        if method not in results[datasets[0]]:\n",
    "            continue\n",
    "        text += f\"\"\"\n",
    "{method_names[method]} & #params \"\"\"\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            if dataset not in results:\n",
    "                continue\n",
    "            r = results[dataset][method]\n",
    "            text += f\"\"\" & {r['test_hv'][0]:.4f} $\\pm$ {r['test_hv'][1]:.4f} & {results[dataset]['single_task']['test_hv'][0] - r['test_hv'][0]:.4f}\"\"\"\n",
    "        \n",
    "        text += \"\"\" \\\\\\\\\"\"\"\n",
    "\n",
    "    with open(f'results_{name}.txt', 'w') as f:\n",
    "        f.writelines(text)\n",
    "\n",
    "\n",
    "def generate_table_taskwise(\n",
    "        results, \n",
    "        datasets=['multi_mnist', 'multi_fashion', 'multi_fashion_mnist'], \n",
    "        methods=['single_task', 'uniform', 'mgda', 'pmtl', 'phn', 'phn_orig', 'cosmos', 'cosmos_orig', ],\n",
    "        name='task'\n",
    "    ):\n",
    "    text = f\"\"\"\"\"\"\n",
    "    for method in methods:\n",
    "        if method not in results[datasets[0]]:\n",
    "            continue\n",
    "\n",
    "        text += f\"\"\"\n",
    "{method_names[method]}\"\"\"\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            if dataset not in results:\n",
    "                continue\n",
    "            r = results[dataset][method]\n",
    "            if len(r['test_scores'][0]) == 25:\n",
    "                r['test_scores'] = (\n",
    "                    r['test_scores'][0][12],   # take the middle point of 25 rays\n",
    "                    r['test_scores'][1][12]\n",
    "                )\n",
    "            elif len(r['test_scores'][0]) == 5:\n",
    "                r['test_scores'] = (\n",
    "                    r['test_scores'][0][2],   # take the middle point of 5 rays\n",
    "                    r['test_scores'][1][2]\n",
    "                )\n",
    "\n",
    "            text += f\"\"\" & {r['test_scores'][0][0]:.4f} $\\pm$ {r['test_scores'][1][0]:.4f} & {r['test_scores'][0][1]:.4f} $\\pm$ {r['test_scores'][1][1]:.4f}\"\"\"\n",
    "        \n",
    "        text += \"\"\" \\\\\\\\\"\"\"\n",
    "\n",
    "    with open(f'results_{name}.txt', 'w') as f:\n",
    "        f.writelines(text)\n",
    "\n",
    "\n",
    "def plot_size_ablation(results, datasets=['multi_mnist', 'multi_fashion', 'multi_fashion_mnist'], prefix=''):\n",
    "    fig, axes = plt.subplots(1, len(datasets), figsize=(4.5*len(datasets), 4))\n",
    "    color_lightness = [1.4, 0.8, 0.6, 0.3]\n",
    "    for j, dataset in enumerate(datasets):\n",
    "        ax = axes[j]\n",
    "        handles = []\n",
    "        for i, size_i in enumerate(['0.5', '1', '10', '50']):\n",
    "            lower_limit = None\n",
    "            r = results[i]\n",
    "            if 'single_task' not in r[dataset]:\n",
    "                continue\n",
    "            st = np.array(r[dataset]['single_task']['test_scores'][0])\n",
    "            uf = np.array(r[dataset]['uniform']['test_scores'][0])\n",
    "\n",
    "            st_err = np.array(r[dataset]['single_task']['test_scores'][1])\n",
    "\n",
    "            ax.annotate('', xy=st, xytext=uf, arrowprops={'arrowstyle': '->'})\n",
    "\n",
    "            ax.errorbar(st[0], st[1], xerr=st_err[0], yerr=st_err[1],\n",
    "                ecolor=adjust_lightness(colors['single_task'], amount=color_lightness[i]),\n",
    "                alpha=.3,\n",
    "                capsize=3)\n",
    "\n",
    "            h = ax.plot(\n",
    "                st[0], \n",
    "                st[1], \n",
    "                color=adjust_lightness(colors['single_task'], amount=color_lightness[i]),\n",
    "                marker='+',\n",
    "                label=f\"{method_names['single_task']} (size {size_i})\",\n",
    "                linestyle=' ',\n",
    "            )\n",
    "            handles.append(h[0])\n",
    "\n",
    "            ax.errorbar(uf[0], uf[1], xerr=st_err[0], yerr=st_err[1],\n",
    "                ecolor=adjust_lightness(colors['uniform'], amount=color_lightness[i]),\n",
    "                alpha=.3,\n",
    "                capsize=3)\n",
    "\n",
    "            h = ax.plot(\n",
    "                uf[0], \n",
    "                uf[1], \n",
    "                color=adjust_lightness(colors['uniform'], amount=color_lightness[i]),\n",
    "                marker=markers['uniform'],\n",
    "                label=f\"{method_names['uniform']} (size {size_i})\",\n",
    "                linestyle=' ',\n",
    "            )\n",
    "            handles.append(h[0])\n",
    "\n",
    "            # ax.text(uf[0]+0.002, uf[1]+0.002, f\"size {size_i}\")\n",
    "        ax.set_title(titles[dataset])\n",
    "        ax_lables = ax_lables_loss[dataset] if 'mcr' not in prefix else ax_lables_mcr[dataset]\n",
    "        ax.set_xlabel(ax_lables[0])\n",
    "        if j==0:\n",
    "            ax.set_ylabel(ax_lables[1])\n",
    "            \n",
    "    fig.subplots_adjust(bottom=0.2, wspace=0.25)\n",
    "    axes[1].legend(handles=handles, loc='upper center', \n",
    "             bbox_to_anchor=(0.5, -0.23),fancybox=False, shadow=False, ncol=4)\n",
    "\n",
    "    # axes[0].legend(loc='upper right')\n",
    "\n",
    "    fig.savefig(prefix + 'ablation.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "928b2361-f283-40ff-97c3-b05535e99fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data for celeba\n"
     ]
    }
   ],
   "source": [
    "results = load_data(dirname='/lfs/local/0/nomir/moo-mtl/results',\n",
    "                    methods=['cosmos', 'cosmos_orig', 'calibrated_linear_scalarization', 'linear_scalarization', 'single_task'],\n",
    "                   custom_metric=True\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae1fe0aa-1d0b-4c98-a5a4-a06ba54c9779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_scores': ([[0.49571195244789124, 0.28653666377067566],\n",
       "   [0.49571195244789124, 0.28653666377067566],\n",
       "   [0.49571195244789124, 0.28653666377067566],\n",
       "   [0.49571195244789124, 0.28653666377067566],\n",
       "   [0.49571195244789124, 0.28653666377067566]],\n",
       "  [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]),\n",
       " 'test_hv': (0.3597910328270997, 0.0),\n",
       " 'train_time': (31203.93132209778, 0.0),\n",
       " 'num_parameters': 17427002}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['celeba']['calibrated_linear_scalarization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09c31276-1f6f-47d6-82f0-71ff0b3f7146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success. See mcr_uncorr.pdf\n"
     ]
    }
   ],
   "source": [
    "plot_row(results, prefix='mcr_uncorr', datasets=['celeba'],\n",
    "        # methods=['single_task', 'cosmos_orig', 'linear_scalarization', 'calibrated_linear_scalarization']\n",
    "         methods=['single_task', 'calibrated_linear_scalarization', 'linear_scalarization', 'cosmos', 'cosmos_orig']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c9371e-2b26-48af-96fb-230e0cc3907b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'celeba': {}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aaa45d0-f5be-45d2-9f6b-df2aea396848",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cosmos_orig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mceleba\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcosmos_orig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cosmos_orig'"
     ]
    }
   ],
   "source": [
    "results['celeba']['cosmos_orig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f64d9fe-42f1-4916-a9ad-d9b793fd524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.random.rand(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6256e4a9-01db-4a7d-84a7-d233ea774744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76971038, 0.22571083, 0.89411608, 0.96852937, 0.56946611],\n",
       "       [0.20862544, 0.38963795, 0.73786994, 0.75414663, 0.31087956],\n",
       "       [0.64088266, 0.64160727, 0.40779501, 0.04913566, 0.20243111],\n",
       "       [0.3473308 , 0.71131234, 0.67087917, 0.79356064, 0.94976201]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f30e22b-cc78-4a86-9715-a1389ab3efb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20862544 0.38963795 0.73786994 0.75414663 0.31087956]\n",
      "[0.64088266 0.64160727 0.40779501 0.04913566 0.20243111]\n",
      "[0.3473308  0.71131234 0.67087917 0.79356064 0.94976201]\n"
     ]
    }
   ],
   "source": [
    "for r in rand[1:]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073b086-d5e6-431d-89b7-764f3077e4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
